{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Razonamiento de los Agentes\n",
    "Este código muestra cómo implementar razonamiento en agentes que interactúan con el entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Setup inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Instalar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai\n",
    "#! pip install tenacity\n",
    "#! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.- Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llm.openai import generate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.- Variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets and config from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "print(\"OpenAI API key: {}\".format(openai.api_key[:5] + '...' + openai.api_key[-5:]))\n",
    "\n",
    "# Model endpoint names\n",
    "gpt35_model = os.getenv(\"OPENAI_GPT35_MODEL\")\n",
    "gpt35_16k_model = os.getenv(\"OPENAI_GPT35_16K_MODEL\")\n",
    "gpt4_model = os.getenv(\"OPENAI_GPT4_MODEL\")\n",
    "print(\"GPT-3.5-Turbo model: {}\".format(gpt35_model))\n",
    "print(\"GPT-3.5-Turbo-16k model: {}\".format(gpt35_16k_model))\n",
    "print(\"GPT-4 model: {}\".format(gpt4_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(messages=[], role=\"user\", input=''):\n",
    "    messages.append({\"role\": role, \"content\": input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Ejemplo sin razonamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Toma la última letra de cada palabra en 'Larry Page' y concaténalas\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model)\n",
    "print(f\"Respuesta Ejemplo 1: {response}\") #Resp: ye\n",
    "\n",
    "user_input = \"Jane programó 3 citas con 5 personas para mañana (martes, 7/9/1972). ¿Cuál es la fecha hace una semana a partir de hoy en formato MM/DD/YYYY?\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model)\n",
    "print(f\"Respuesta Ejemplo 2: {response}\") #Resp:  07/01/1972"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Chain Of Thought (CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT Prompt\n",
    "COT_PROMPT = \"Piensa detenida y lógicamente, explicando tu respuesta paso a paso.\"\n",
    "\n",
    "user_input = f\"Toma la última letra de cada palabra en 'Larry Page' y concaténalas. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model)\n",
    "print(f\"Respuesta Ejemplo 1: {response}\\n\") #Resp: ye\n",
    "\n",
    "# En este ejemplo no es suficiente un prompt simple\n",
    "user_input = f\"Jane programó 3 citas con 5 personas para mañana (martes, 7/9/1972). ¿Cuál es la fecha hace una semana a partir de hoy en formato MM/DD/YYYY?. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model)\n",
    "print(f\"Respuesta Ejemplo 2: {response}\\n\") #Resp:  07/01/1972"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Chain Of Thought (CoT) + Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COT_PROMPT = \"Piensa detenida y lógicamente, explicando tu respuesta paso a paso.\"\n",
    "\n",
    "# Inyeccion de mensajes de ejemplo\n",
    "messages=[]\n",
    "#Example 1: Math problem\n",
    "add_message(messages,\"user\",\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\")\n",
    "add_message(messages,\"assistant\",\" There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\")\n",
    "#Example 2: Concadenate problem\n",
    "add_message(messages,\"user\",\"Take the last letters of the words in 'Elon Musk' and concatenate them.\")\n",
    "add_message(messages,\"assistant\",\" The last letter of 'Elon' is 'n'. The last letter of 'Musk' is 'k'. Concatenating them is 'nk'. The answer is nk.\")\n",
    "#Example 3:  CSQA problem\n",
    "add_message(messages,\"user\",\"What do people use to absorb extra ink from a fountain pen? Answer Choices: (a) shirt pocket (b) calligrapher’s hand (c) inkwell (d) desk drawer (e) blotter\")\n",
    "add_message(messages,\"assistant\",\"The answer must be an item that can absorb ink. Of the above choices, only blotters are used to absorb ink.So the answer is (e).\")\n",
    "#Example 4:  Date Understanding problem\n",
    "add_message(messages,\"user\",\" Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY?\")\n",
    "add_message(messages,\"assistant\",\"The last day of February is the 28th, so Jane was born on 02/28/2001. Today is her 16-year old birthday, so today is 02/28/2017. So yesterday was 02/27/2017. So the answer is 02/27/2017.\")\n",
    "\n",
    "# Se ejecuta 1 peticion\n",
    "user_input = f\"Toma la última letra de cada palabra en 'Larry Page' y concaténalas. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model,messages=messages)\n",
    "print(f\"Respuesta Ejemplo 1: {response}\\n\") #Resp: ye\n",
    "\n",
    "# Se ejecuta 2 peticion\n",
    "user_input = f\"Michael tenía 58 pelotas de golf. El martes perdió 23 pelotas de golf. El miércoles perdió 2 más. ¿Cuántas pelotas de golf tenía al final del miércoles?. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model,messages=messages)\n",
    "print(f\"\\nRespuesta Ejemplo 2: {response}\\n\") #Resp: 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.- CoT como un Tree Of Thought (ToT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TREE_SOLUTIONS=3\n",
    "\n",
    "PROMPT_COT_LIKE_TOT = f\"\"\"\n",
    "Simula {N_TREE_SOLUTIONS} expertos lógicos brillantes que responden colaborativamente a una pregunta.\n",
    "Todos los expertos escribirán un paso de pensamiento logico y luego lo compartirán con el grupo.\n",
    "Se prosigue generando un nuevo paso de pensamiento logico hasta encontrar un concenso.\n",
    "Se finaliza respondiendo la pregunta deacuerdo con la conclusion final.\n",
    "\"\"\"\n",
    "\n",
    "user_input = f\"\"\"\n",
    "Bob está en la sala de estar.\n",
    "Camina hacia la cocina, llevando una taza.\n",
    "Pone una pelota en la taza y la lleva al dormitorio.\n",
    "Le da la vuelta a la taza y luego camina hacia el jardín.\n",
    "Deja la taza en el jardín y luego camina hacia el garaje.\n",
    "¿Donde está la pelota? \n",
    "\"\"\"\n",
    "# Respuesta: La pelota está en el dormitorio.\n",
    "\n",
    "messages=[]\n",
    "add_message(messages,\"system\",f\"{PROMPT_COT_LIKE_TOT}\")\n",
    "\n",
    "response = generate_text(prompt=user_input,model=gpt4_model,messages=messages,max_tokens=2000)\n",
    "print(f\"{response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.- Tree Of Thought (ToT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SOLUTIONS=3\n",
    "STEPS=2\n",
    "\n",
    "PROMPT_TOT_GENERATOR=f\"\"\"\n",
    "I have a problem related to the user request.\n",
    "Generate {N_SOLUTIONS} differents logical and common sense experts to response only one thought in one step.\n",
    "Please considere a variety of factors such as the user request, the context, the environment, etc.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TOT_STATE_EVALUATOR=f\"\"\"\n",
    "For each step solution evaluate the posibility to reach the solution form user request and rank each step solution with confidence score.\n",
    "Also reply with the most likely solution to get the solution.\n",
    "\"\"\"\n",
    "\n",
    "user_input = \"\"\"\n",
    "Tom dejo 3 calcetines en la lavadora, al finalizar, dejo los 3 calcetines secar, luego de 10 horas los calcetines se secaron completamente. ¿Cuánto tiempo llevara secar 20 calcetines?\n",
    "\"\"\"\n",
    "\n",
    "response=user_input\n",
    "for i in range(STEPS):\n",
    "    print(f\"----------------------> Step {i+1}:\\n\")\n",
    "    print(messages)\n",
    "    # 1.- Descomposición del pensamiento\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_GENERATOR}\")\n",
    "    if i is not 0:\n",
    "        add_message(messages,\"user\",f\"{user_input}\")\n",
    "    response = generate_text(prompt=response,model=gpt4_model,messages=messages,max_tokens=1000)\n",
    "    add_message(messages,\"assistant\",f\"{response}\")\n",
    "    print(f\"----> DESCOMPOSITION:\\n{response}\\n\")\n",
    "\n",
    "    # 2.- Evaluación de pensamiento\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_STATE_EVALUATOR}\")\n",
    "    add_message(messages,\"user\",f\"{user_input}\")\n",
    "    response = generate_text(prompt=response,model=gpt4_model,messages=messages,max_tokens=1000)\n",
    "    add_message(messages,\"assistant\",f\"{response}\")\n",
    "    print(f\"----> EVALUATION:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.- Tree Of Thought (ToT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SOLUTIONS=3\n",
    "\n",
    "PROMPT_TOT_BRAINSTROM=f\"\"\"\n",
    "I have a problem related to the user request. Could you brainstorm {N_SOLUTIONS} distinct solutions? Please considere a variety of factors such as the user request, the context, the environment, etc.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TOT_EVALUATION=f\"\"\"\n",
    "For each of {N_SOLUTIONS} proposed solutions, calculate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assing a probability of success and a confidence level to each option based on these factors.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TOT_EXPANSION=f\"\"\"\n",
    "For each solution deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TOT_DECISION=f\"\"\"\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution.\n",
    "\"\"\"\n",
    "\n",
    "user_input = \"\"\"\n",
    "Tom dejo 3 calcetines en la lavadora, al finalizar, dejo los 3 calcetines secar, luego de 10 horas los calcetines se secaron completamente. ¿Cuánto tiempo llevara secar 20 calcetines?\n",
    "\"\"\"\n",
    "\n",
    "response=user_input\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "    print(f\"----------------------> Step {i+1}:\\n\")\n",
    "    # 1.- Generador de Ideas\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_BRAINSTROM}\")\n",
    "    response = generate_text(prompt=response,model=gpt35_model,messages=messages,max_tokens=1000)\n",
    "    print(f\"----> BRAINSTROM:\\n{response}\\n\")\n",
    "\n",
    "    # 2.- Evaluador de pensamiento\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_EVALUATION}\")\n",
    "    response = generate_text(prompt=response,model=gpt35_model,messages=messages,max_tokens=1000)\n",
    "    print(f\"----> EVALUATION:\\n{response}\\n\")\n",
    "\n",
    "    # 3.- Expansor de pensamiento\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_EXPANSION}\")\n",
    "    response = generate_text(prompt=response,model=gpt35_model,messages=messages,max_tokens=1000)\n",
    "    print(f\"----> EXPANSION:\\n{response}\\n\")\n",
    "\n",
    "    # 4.- Decision de pensamiento\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_DECISION}\")\n",
    "    response = generate_text(prompt=response,model=gpt35_model,messages=messages,max_tokens=1000)\n",
    "    print(f\"----> DECISION:\\n{response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
