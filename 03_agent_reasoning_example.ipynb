{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Razonamiento de los Agentes\n",
    "Este código muestra cómo implementar razonamiento en agentes que interactúan con el entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Setup inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Instalar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.- Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llm import generate_text, gpt4_model\n",
    "from agents.memory import LastMemories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.- Variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets and config from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "print(\"OpenAI API key: {}\".format(openai.api_key[:5] + '...' + openai.api_key[-5:]))\n",
    "\n",
    "# Model endpoint names\n",
    "gpt35_model = os.getenv(\"OPENAI_GPT35_MODEL\")\n",
    "gpt35_16k_model = os.getenv(\"OPENAI_GPT35_16K_MODEL\")\n",
    "print(\"GPT-3.5-Turbo model: {}\".format(gpt35_model))\n",
    "print(\"GPT-3.5-Turbo-16k model: {}\".format(gpt35_16k_model))\n",
    "print(\"GPT-4 model: {}\".format(gpt35_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Tecnicas de Razonamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Ejemplos sin razonamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"Toma la última letra de cada palabra en 'Larry Page' y concaténalas.\"\n",
    "response = generate_text(user_input)\n",
    "print(response) #Resp: ye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"Tom dejo 3 calcetines en la lavadora, al finalizar, dejo los 3 calcetines secar, luego de 10 horas los calcetines se secaron completamente. ¿Cuánto tiempo llevara secar 20 calcetines?.\"\n",
    "response = generate_text(user_input)\n",
    "print(response) #Resp: 10 Horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"Jane nació el último día de febrero de 2001. Hoy es su cumpleaños de 16 años. Cual es la fecha ayer en DD/MM/YYYY?\"\n",
    "response = generate_text(user_input)\n",
    "print(response) #Resp: 27/02/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Chain Of Thought (CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT Prompt\n",
    "COT_PROMPT = \"Piensa con sentido comun, detenida y lógicamente, explicando tu respuesta paso a paso.\"\n",
    "\n",
    "user_input = f\"Toma la última letra de cada palabra en 'Larry Page' y concaténalas. {COT_PROMPT}\"\n",
    "response = generate_text(user_input)\n",
    "print(response) #Resp: ye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"Tom dejo 3 calcetines en la lavadora, al finalizar, dejo los 3 calcetines secar, luego de 10 horas los calcetines se secaron completamente. ¿Cuánto tiempo llevara secar 20 calcetines?. {COT_PROMPT}\"\n",
    "response = generate_text(user_input)\n",
    "print(response) #Resp: 10 Horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"Jane nació el último día de febrero de 2001. Hoy es su cumpleaños de 16 años. Cual es la fecha ayer en DD/MM/YYYY?. {COT_PROMPT}\"\n",
    "response = generate_text(user_input)\n",
    "print(response) #Resp: 27/02/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Chain Of Thought (CoT) + Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COT_PROMPT = \"Piensa con sentido comun, detenida y lógicamente, explicando tu respuesta paso a paso.\"\n",
    "\n",
    "# Inyeccion de mensajes de ejemplo\n",
    "messages = LastMemories()\n",
    "\n",
    "#Example 1: Concadenate problem\n",
    "messages.add_to_memory(\"user\",\"Take the last letters of the words in 'Elon Musk' and concatenate them.\")\n",
    "messages.add_to_memory(\"assistant\",\"The last letter of 'Elon' is 'n'. The last letter of 'Musk' is 'k'. Concatenating them is 'nk'. The answer is nk.\")\n",
    "#Example 2:  Date Understanding problem\n",
    "messages.add_to_memory(\"user\",\"The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?\")\n",
    "messages.add_to_memory(\"assistant\",\"One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the answer is 05/23/1943.\")\n",
    "# Example 3: Date Understanding problem\n",
    "messages.add_to_memory(\"user\",\"Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY?\")\n",
    "messages.add_to_memory(\"assistant\",\"Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. So the answer is 03/13/2002.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"Toma la última letra de cada palabra en 'Larry Page' y concaténalas. {COT_PROMPT}\"\n",
    "response = generate_text(user_input, messages=messages.get_last_memories(10))\n",
    "print(response) #Resp: ye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"Tom dejo 3 calcetines en la lavadora, al finalizar, dejo los 3 calcetines secar, luego de 10 horas los calcetines se secaron completamente. ¿Cuánto tiempo llevara secar 20 calcetines?. {COT_PROMPT}\"\n",
    "response = generate_text(user_input, messages=messages.get_last_memories(10))\n",
    "print(response) #Resp: 10 Horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"Jane nació el último día de febrero de 2001. Hoy es su cumpleaños de 16 años. Cual es la fecha ayer en MM/DD/YYYY?. {COT_PROMPT}\"\n",
    "response = generate_text(user_input, messages=messages.get_last_memories(10))\n",
    "print(response) #Resp: 02/27/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.- Tree Of Thought (ToT) simplifacado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SOLUTIONS=5\n",
    "STEPS=3\n",
    "\n",
    "PROMPT_TOT_GENERATOR = f\"\"\"Based on the <QUERY>, generate {N_SOLUTIONS} different logical and common sense experts and consider type of experts in the area of the <QUERY> keep in mind the context, the environment, etc.\n",
    "Response only one step (one different for each expert), in one short sentence, to solve the user <QUERY>.\"\"\"\n",
    "\n",
    "PROMPT_TOT_STATE_EVALUATOR=f\"\"\"\n",
    "For each expert <SOLUTION> evaluate the posibility to reach the solution from user <QUERY> and rank each step solution with confidence score. Consider as most relevant the expert who most closely resembles the area of <QUERY>. Reply with a list of each solution and its confidence score. The confidence score should be between 0 and 100%.\n",
    "\"\"\"\n",
    "PROMPT_TOT_EXIT_CONDITION= \"Review the <EVALUATION> about the user <QUERY>. If the confidence score of the most likely solution is greater than 90%, then reply only with 'STOP'.\"\n",
    "\n",
    "user_input = \"\"\"Tom dejo 3 calcetines en la lavadora, al finalizar, dejo los 3 calcetines secar, luego de 10 horas los calcetines se secaron completamente. ¿Cuánto tiempo llevara secar 20 calcetines?\"\"\"\n",
    "\n",
    "messages = LastMemories()\n",
    "response = user_input\n",
    "for i in range(STEPS):\n",
    "    print(f\"----------------------> Step {i+1}:\\n\")\n",
    "    # 1.- Descomposición del pensamiento\n",
    "    messages.clean_memory()\n",
    "    messages.add_to_memory(\"system\", PROMPT_TOT_GENERATOR)\n",
    "    if i != 0:\n",
    "        messages.add_to_memory(\"user\",f\"{user_input}\")\n",
    "    solutions = generate_text(response, messages=messages.get_last_memories(n=20))\n",
    "    print(f\"----> SOLUTIONS:\\n{solutions}\\n\")\n",
    "\n",
    "    # 2.- Evaluación de pensamiento\n",
    "    messages.clean_memory()\n",
    "    messages.add_to_memory(\"system\", PROMPT_TOT_STATE_EVALUATOR)\n",
    "    evaluation = generate_text(f\"<QUERY>\\n{user_input}\\n\\n<SOLUTIONS>\\n{solutions}\", messages=messages.get_last_memories(n=20))\n",
    "    print(f\"----> EVALUATION:\\n{evaluation}\\n\")\n",
    "    \n",
    "    # 3.- Condicion de salida\n",
    "    messages.clean_memory()\n",
    "    messages.add_to_memory(\"system\", PROMPT_TOT_EXIT_CONDITION)\n",
    "    response = generate_text(f\"<QUERY>\\n{user_input}\\n\\n<EVALUATION>\\n{evaluation}\", messages=messages.get_last_memories(n=20))\n",
    "    if response == \"STOP\":\n",
    "        print(f\"----> EXIT CONDITION:\\n{response}\\n\")\n",
    "        messages.clean_memory()\n",
    "        messages.add_to_memory(\"system\", \"Based on the user <QUERY> and the <SOLUTIONS> and <EVALUATION>, give a final answer based on the solution with the best confidence score.\")\n",
    "        final_answer = generate_text(f\"<QUERY>\\n{user_input}\\n\\n<SOLUTIONS>\\n{solutions}\\n\\n<EVALUATION>\\n{evaluation}\", messages=messages.get_last_memories(n=20))\n",
    "        print(final_answer)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
