{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Razonamiento de los Agentes\n",
    "Este código muestra cómo implementar razonamiento en agentes que interactúan con el entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Setup inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Instalar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai\n",
    "#! pip install tenacity\n",
    "#! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.- Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llm.openai import generate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.- Variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets and config from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "print(\"OpenAI API key: {}\".format(openai.api_key[:5] + '...' + openai.api_key[-5:]))\n",
    "\n",
    "# Model endpoint names\n",
    "gpt35_model = os.getenv(\"OPENAI_GPT35_MODEL\")\n",
    "gpt35_16k_model = os.getenv(\"OPENAI_GPT35_16K_MODEL\")\n",
    "print(\"GPT-3.5-Turbo model: {}\".format(gpt35_model))\n",
    "print(\"GPT-3.5-Turbo-16k model: {}\".format(gpt35_16k_model))\n",
    "print(\"GPT-4 model: {}\".format(gpt35_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.- Metodo de utilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(messages=[], role=\"user\", input=''):\n",
    "    messages.append({\"role\": role, \"content\": input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Ejemplo sin razonamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ejecuta 1 peticion\n",
    "user_input = f\"Toma la última letra de cada palabra en 'Larry Page' y concaténalas.\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model)\n",
    "print(f\"Respuesta Ejemplo 1: {response}\\n\") #Resp: ye\n",
    "\n",
    "# Se ejecuta 2 peticion\n",
    "user_input = f\"Tom dejo 3 calcetines en la lavadora, al finalizar, dejo los 3 calcetines secar, luego de 10 horas los calcetines se secaron completamente. ¿Cuánto tiempo llevara secar 20 calcetines?.\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model)\n",
    "print(f\"\\nRespuesta Ejemplo 2: {response}\\n\") #Resp: 10 Horas\n",
    "\n",
    "# Se ejecuta 3 peticion\n",
    "user_input = f\"Jane nació el último día de febrero de 2001. Hoy es su cumpleaños de 16 años. Cual es la fecha ayer en MM/DD/YYYY?\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model,max_tokens=1000)\n",
    "print(f\"\\nRespuesta Ejemplo 3: {response}\\n\") #Resp: 02/27/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Chain Of Thought (CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT Prompt\n",
    "COT_PROMPT = \"Piensa con sentido comun, detenida y lógicamente, explicando tu respuesta paso a paso.\"\n",
    "\n",
    "# Se ejecuta 1 peticion\n",
    "user_input = f\"Toma la última letra de cada palabra en 'Larry Page' y concaténalas. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model)\n",
    "print(f\"Respuesta Ejemplo 1: {response}\\n\") #Resp: ye\n",
    "\n",
    "# Se ejecuta 2 peticion\n",
    "user_input = f\"Tom dejo 3 calcetines en la lavadora, al finalizar, dejo los 3 calcetines secar, luego de 10 horas los calcetines se secaron completamente. ¿Cuánto tiempo llevara secar 20 calcetines?. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model)\n",
    "print(f\"\\nRespuesta Ejemplo 2: {response}\\n\") #Resp: 10 Horas\n",
    "\n",
    "# Se ejecuta 3 peticion\n",
    "user_input = f\"Jane nació el último día de febrero de 2001. Hoy es su cumpleaños de 16 años. Cual es la fecha ayer en MM/DD/YYYY?. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model,max_tokens=1000)\n",
    "print(f\"\\nRespuesta Ejemplo 3: {response}\\n\") #Resp: 02/27/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Chain Of Thought (CoT) + Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COT_PROMPT = \"Piensa con sentido comun, detenida y lógicamente, explicando tu respuesta paso a paso.\"\n",
    "\n",
    "# Inyeccion de mensajes de ejemplo\n",
    "messages=[]\n",
    "\n",
    "#Example 1: Concadenate problem\n",
    "add_message(messages,\"user\",\"Take the last letters of the words in 'Elon Musk' and concatenate them.\")\n",
    "add_message(messages,\"assistant\",\" The last letter of 'Elon' is 'n'. The last letter of 'Musk' is 'k'. Concatenating them is 'nk'. The answer is nk.\")\n",
    "#Example 2:  Date Understanding problem\n",
    "add_message(messages,\"user\",\"The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?\")\n",
    "add_message(messages,\"assistant\",\"One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the answer is 05/23/1943.\")\n",
    "add_message(messages,\"user\",\"Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY?\")\n",
    "add_message(messages,\"assistant\",\"Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY?\")\n",
    "add_message(messages,\"user\",\"Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY?\")\n",
    "add_message(messages,\"assistant\",\"Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. So the answer is 03/13/2002.\")\n",
    "\n",
    "# Se ejecuta 1 peticion\n",
    "user_input = f\"Toma la última letra de cada palabra en 'Larry Page' y concaténalas. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model)\n",
    "print(f\"Respuesta Ejemplo 1: {response}\\n\") #Resp: ye\n",
    "\n",
    "# Se ejecuta 2 peticion\n",
    "user_input = f\"Tom dejo 3 calcetines en la lavadora, al finalizar, dejo los 3 calcetines secar, luego de 10 horas los calcetines se secaron completamente. ¿Cuánto tiempo llevara secar 20 calcetines?. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model,max_tokens=1000)\n",
    "print(f\"\\nRespuesta Ejemplo 2: {response}\\n\") #Resp: 10 Horas\n",
    "\n",
    "# Se ejecuta 3 peticion\n",
    "user_input = f\"Jane nació el último día de febrero de 2001. Hoy es su cumpleaños de 16 años. Cual es la fecha ayer en MM/DD/YYYY?. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model,max_tokens=1000)\n",
    "print(f\"\\nRespuesta Ejemplo 3: {response}\\n\") #Resp: 02/27/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.- CoT como un Tree Of Thought (ToT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TREE_SOLUTIONS=3\n",
    "\n",
    "PROMPT_COT_LIKE_TOT = f\"\"\"\n",
    "Simula {N_TREE_SOLUTIONS} expertos lógicos brillantes que responden colaborativamente a una pregunta.\n",
    "Todos los expertos escribirán un paso de pensamiento logico y luego lo compartirán con el grupo.\n",
    "Se prosigue generando un nuevo paso de pensamiento logico hasta encontrar un concenso.\n",
    "Se finaliza respondiendo la pregunta deacuerdo con la conclusion final.\n",
    "\"\"\"\n",
    "\n",
    "user_input = f\"\"\"\n",
    "Bob está en la sala de estar.\n",
    "Camina hacia la cocina, llevando una taza.\n",
    "Pone una pelota en la taza y la lleva al dormitorio.\n",
    "Le da la vuelta a la taza, boca abajo, y luego camina hacia el jardín.\n",
    "Deja la taza en el jardín y luego camina hacia el garaje.\n",
    "¿Donde está la pelota? \n",
    "\"\"\"\n",
    "# Respuesta: La pelota está en el dormitorio.\n",
    "\n",
    "messages=[]\n",
    "add_message(messages,\"system\",f\"{PROMPT_COT_LIKE_TOT}\")\n",
    "\n",
    "# Se ejecuta 1 peticion\n",
    "response = generate_text(prompt=user_input,model=gpt35_model,messages=messages,max_tokens=2000)\n",
    "print(f\"{response}\")\n",
    "\n",
    "# Se ejecuta 2 peticion\n",
    "user_input = f\"Toma la última letra de cada palabra en 'Larry Page' y concaténalas. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model)\n",
    "print(f\"Respuesta Ejemplo 1: {response}\\n\") #Resp: ye\n",
    "\n",
    "# Se ejecuta 3 peticion\n",
    "user_input = f\"Tom dejo 3 calcetines en la lavadora, al finalizar, dejo los 3 calcetines secar, luego de 10 horas los calcetines se secaron completamente. ¿Cuánto tiempo llevara secar 20 calcetines?. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model)\n",
    "print(f\"\\nRespuesta Ejemplo 2: {response}\\n\") #Resp: 10 Horas\n",
    "\n",
    "# Se ejecuta 4 peticion\n",
    "user_input = f\"El primer día de 2019 es martes y hoy es el primer lunes de 2019. ¿Cuál es la fecha de hoy en MM/DD/AAAA?. {COT_PROMPT}\"\n",
    "response = generate_text(prompt=user_input,model=gpt35_model,max_tokens=1000)\n",
    "print(f\"\\nRespuesta Ejemplo 3: {response}\\n\") #Resp:  01/07/2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.- Tree Of Thought (ToT) - Generador y Evaluador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SOLUTIONS=3\n",
    "STEPS=2\n",
    "\n",
    "PROMPT_TOT_GENERATOR=f\"\"\"\n",
    "I have a problem related to the user request.\n",
    "Generate {N_SOLUTIONS} differents logical and common sense experts to response only one thought in one step.\n",
    "Please considere a variety of factors such as the user request, the context, the environment, etc.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TOT_STATE_EVALUATOR=f\"\"\"\n",
    "For each step solution evaluate the posibility to reach the solution form user request and rank each step solution with confidence score.\n",
    "Also reply with the most likely solution to get the solution.\n",
    "\"\"\"\n",
    "\n",
    "user_input = \"\"\"\n",
    "Tom dejo 3 calcetines en la lavadora, al finalizar, dejo los 3 calcetines secar, luego de 10 horas los calcetines se secaron completamente. ¿Cuánto tiempo llevara secar 20 calcetines?\n",
    "\"\"\"\n",
    "\n",
    "response=user_input\n",
    "for i in range(STEPS):\n",
    "    print(f\"----------------------> Step {i+1}:\\n\")\n",
    "    # 1.- Descomposición del pensamiento\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_GENERATOR}\")\n",
    "    if i != 0:\n",
    "        add_message(messages,\"user\",f\"{user_input}\")\n",
    "    response = generate_text(prompt=response,model=gpt35_model,messages=messages,max_tokens=1000)\n",
    "    add_message(messages,\"assistant\",f\"{response}\")\n",
    "    print(f\"----> DESCOMPOSITION:\\n{response}\\n\")\n",
    "\n",
    "    # 2.- Evaluación de pensamiento\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_STATE_EVALUATOR}\")\n",
    "    add_message(messages,\"user\",f\"{user_input}\")\n",
    "    response = generate_text(prompt=response,model=gpt35_model,messages=messages,max_tokens=1000)\n",
    "    add_message(messages,\"assistant\",f\"{response}\")\n",
    "    print(f\"----> EVALUATION:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.- Tree Of Thought (ToT) 4 Fases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SOLUTIONS=3\n",
    "STEPS=1\n",
    "\n",
    "PROMPT_TOT_BRAINSTROM=f\"\"\"\n",
    "I have a problem related to the user request. Could you brainstorm {N_SOLUTIONS} distinct solutions? Please considere a variety of factors such as the user request, the context, the environment, etc.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TOT_EVALUATION=f\"\"\"\n",
    "For each of {N_SOLUTIONS} proposed solutions, calculate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assing a probability of success and a confidence level to each option based on these factors.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TOT_EXPANSION=f\"\"\"\n",
    "For each solution deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TOT_DECISION=f\"\"\"\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution.\n",
    "\"\"\"\n",
    "\n",
    "user_input = \"\"\"\n",
    "Tengo un problema para determinar la mejor forma para construir una casa. Que me recomendarías para comenzar?\n",
    "\"\"\"\n",
    "\n",
    "response=user_input\n",
    "\n",
    "for i in range(STEPS):\n",
    "\n",
    "    print(f\"----------------------> Step {i+1}:\\n\")\n",
    "    # 1.- Generador de Ideas\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_BRAINSTROM}\")\n",
    "    response = generate_text(prompt=response,model=gpt35_model,messages=messages,max_tokens=1000)\n",
    "    print(f\"----> BRAINSTROM:\\n{response}\\n\")\n",
    "\n",
    "    # 2.- Evaluador de pensamiento\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_EVALUATION}\")\n",
    "    response = generate_text(prompt=response,model=gpt35_model,messages=messages,max_tokens=1000)\n",
    "    print(f\"----> EVALUATION:\\n{response}\\n\")\n",
    "\n",
    "    # 3.- Expansor de pensamiento\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_EXPANSION}\")\n",
    "    response = generate_text(prompt=response,model=gpt35_model,messages=messages,max_tokens=1000)\n",
    "    print(f\"----> EXPANSION:\\n{response}\\n\")\n",
    "\n",
    "    # 4.- Decision de pensamiento\n",
    "    messages=[]\n",
    "    add_message(messages,\"system\",f\"{PROMPT_TOT_DECISION}\")\n",
    "    response = generate_text(prompt=response,model=gpt35_model,messages=messages,max_tokens=1000)\n",
    "    print(f\"----> DECISION:\\n{response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
