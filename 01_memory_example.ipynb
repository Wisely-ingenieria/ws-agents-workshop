{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria\n",
    "Este código está diseñado para ilustrar y comparar tres funciones de memoria diferentes utilizadas en sistemas de IA o chatbots: **<span style=\"color:green\">LastMemories</span>**, **<span style=\"color:green\">LastTokens</span>** y **<span style=\"color:green\">RelevantMemories</span>**. Cada una de estas funciones de memoria cumple un propósito distinto en la gestión y recuperación del historial de conversaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Setup inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Instalar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai\n",
    "#! pip install numpy\n",
    "#! pip install tenacity\n",
    "#! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.- Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy\n",
    "from datetime import datetime\n",
    "from llm import generate_text\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.- Variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets and config from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "print(\"OpenAI API key: {}\".format(openai.api_key[:5] + '...' + openai.api_key[-5:]))\n",
    "\n",
    "# Model endpoint names\n",
    "gpt35_model = os.getenv(\"OPENAI_GPT35_MODEL\")\n",
    "gpt35_16k_model = os.getenv(\"OPENAI_GPT35_16K_MODEL\")\n",
    "gpt4_model = os.getenv(\"OPENAI_GPT4_MODEL\")\n",
    "print(\"GPT-3.5-Turbo model: {}\".format(gpt35_model))\n",
    "print(\"GPT-3.5-Turbo-16k model: {}\".format(gpt35_16k_model))\n",
    "print(\"GPT-4 model: {}\".format(gpt4_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.- Importar e inicializar clases a comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.memory.last_memories import LastMemories\n",
    "from agents.memory.last_tokens import LastTokens\n",
    "from agents.memory.relevant_memories import RelevantMemories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Definición de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.- Función para generar una entrada del usuario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_input(model, conversation):\n",
    "    if len(conversation) > 0 and len(conversation) % 4 == 0:\n",
    "        prompt = f\"Basado en esta conversación: {conversation}, genéra una prompt con más preguntas, que sean interesantes sobre el mismo tema.\"\n",
    "        response = generate_text(prompt, model=model, messages=conversation)\n",
    "    else:\n",
    "        prompt = \"Genera una prompt sobre una tema interesante del que se pueda conversar.\"\n",
    "        response = generate_text(prompt, model=model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.- Función para simular una conversación y mostrarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_and_display_conversation(model):\n",
    "    conversation = []\n",
    "\n",
    "    for i in range(10):  # Simular una conversación de 10 turnos\n",
    "        user_input = generate_user_input(model, conversation)\n",
    "        response = generate_text(user_input, model=model, messages=conversation)\n",
    "        \n",
    "        conversation.append({\"role\": \"user\", \"content\" : user_input})\n",
    "        conversation.append({\"role\": \"assistant\", \"content\" : response})\n",
    "            \n",
    "    # Mostrar la conversación en formato HTML\n",
    "    display(HTML(\"<h3>Conversación generada:</h3>\"))\n",
    "    display(HTML('<table><tr><th>Rol</th><th>Texto</th></tr>' + ''.join([f'<tr><td>{d[\"role\"]}</td><td>{d[\"content\"]}</td></tr>' for d in lista_de_diccionarios]) + '</table>'))\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Generar conversación con los distintos modelos y almacenarlas en las respectivas clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.- Instanciar las clases para los distintos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-Turbo\n",
    "last_memories_3 = LastMemories()\n",
    "last_tokens_3 = LastTokens()\n",
    "relevant_memories_3 = RelevantMemories()\n",
    "\n",
    "# GPT-3.5-Turbo-16K\n",
    "last_memories_3_16k = LastMemories()\n",
    "last_tokens_3_16k = LastTokens()\n",
    "relevant_memories_3_16k = RelevantMemories()\n",
    "\n",
    "# GPT-4\n",
    "last_memories_4 = LastMemories()\n",
    "last_tokens_4 = LastTokens()\n",
    "relevant_memories_4 = RelevantMemories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.- GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = simulate_and_display_conversation(gpt35_model)\n",
    "for message in conversation:\n",
    "    last_memories_3.add_to_memory(message[\"role\"], message[\"content\"])\n",
    "    last_tokens_3.add_to_memory(message[\"role\"], message[\"content\"])\n",
    "    relevant_memories_3.add_to_memory(message[\"role\"], message[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.- GPT-3.5-Turbo-16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = simulate_and_display_conversation(gpt35_model)\n",
    "for message in conversation:\n",
    "    last_memories_3_16k.add_to_memory(message[\"role\"], message[\"content\"])\n",
    "    last_tokens_3_16k.add_to_memory(message[\"role\"], message[\"content\"])\n",
    "    relevant_memories_3_16k.add_to_memory(message[\"role\"], message[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.- GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = simulate_and_display_conversation(gpt35_model)\n",
    "for message in conversation:\n",
    "    last_memories_4.add_to_memory(message[\"role\"], message[\"content\"])\n",
    "    last_tokens_4.add_to_memory(message[\"role\"], message[\"content\"])\n",
    "    relevant_memories_4.add_to_memory(message[\"role\"], message[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Comparación de las funciones de Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.- Last Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<pre>' + '\\n'.join(last_memories_3.get_last_memories()) + '</pre>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<pre>' + '\\n'.join(last_memories_3_16k.get_last_memories()) + '</pre>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<pre>' + '\\n'.join(last_memories_4.get_last_memories()) + '</pre>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.- Last Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.- Relevant Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
