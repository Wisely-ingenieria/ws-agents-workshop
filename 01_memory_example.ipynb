{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria\n",
    "Este código está diseñado para ilustrar y comparar tres funciones de memoria diferentes utilizadas en sistemas de IA o chatbots: **<span style=\"color:green\">LastMemories</span>**, **<span style=\"color:green\">LastTokens</span>** y **<span style=\"color:green\">RelevantMemories</span>**. Cada una de estas funciones de memoria cumple un propósito distinto en la gestión y recuperación del historial de conversaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Setup inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Instalar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai\n",
    "#! pip install tenacity\n",
    "#! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.- Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from agents.memory.relevant_memories import RelevantMemories\n",
    "from llm import generate_text, count_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.- Variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets and config from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OpenAI API key: {}\".format(openai.api_key[:5] + '...' + openai.api_key[-5:]))\n",
    "\n",
    "# Model endpoint names\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "gpt35_model = os.getenv(\"OPENAI_GPT35_MODEL\")\n",
    "gpt35_16k_model = os.getenv(\"OPENAI_GPT35_16K_MODEL\")\n",
    "gpt4_model = os.getenv(\"OPENAI_GPT4_MODEL\")\n",
    "print(\"GPT-3.5-Turbo model: {}\".format(gpt35_model))\n",
    "print(\"GPT-3.5-Turbo-16k model: {}\".format(gpt35_16k_model))\n",
    "print(\"GPT-4 model: {}\".format(gpt4_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Ejemplo de uso de las funciones de memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.- Importación de conversacion de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = RelevantMemories()\n",
    "\n",
    "with open(\"conversation.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    loaded_file = json.load(json_file)\n",
    "    conversation = loaded_file[\"conversation\"]\n",
    "for message in conversation:\n",
    "    memory.add_to_memory(message[\"role\"], message[\"content\"])\n",
    "    print(\"\\033[1m{:<12}\\033[0m: {}\".format(message[\"role\"], message[\"content\"]))\n",
    "  \n",
    "print(\"\\n\\n\")  \n",
    "\n",
    "tokens = 0\n",
    "for message in conversation:\n",
    "    tokens += count_tokens(message[\"content\"])\n",
    "print(f'Total de tokens de la conversación: {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.- Last Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories = memory.get_last_memories()\n",
    "for message in memories:\n",
    "    print(\"\\033[1m[{}]\\t{:10}\\033[0m: {}\".format(message[\"timestamp\"].strftime(\"%Y-%m-%d %H:%M:%S\"), message[\"role\"], message[\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.- Last Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories = memory.get_last_tokens(max_tokens=200)\n",
    "for message in memories:\n",
    "    print(\"\\033[1m[{}]\\t{:10}\\033[0m: {}\".format(message[\"timestamp\"].strftime(\"%Y-%m-%d %H:%M:%S\"), message[\"role\"], message[\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.- Relevant Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = memory.get_relevant_memories(\"deportes\", max_tokens=200, sim_k=0.8)\n",
    "for message in relevant:\n",
    "    print(\"\\033[1m[{}]\\t{:10}\\033[0m: {}\".format(message[\"timestamp\"].strftime(\"%Y-%m-%d %H:%M:%S\"), message[\"role\"], message[\"content\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
