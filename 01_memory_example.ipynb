{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria\n",
    "Este código está diseñado para ilustrar y comparar tres funciones de memoria diferentes utilizadas en sistemas de IA o chatbots: **<span style=\"color:green\">LastMemories</span>**, **<span style=\"color:green\">LastTokens</span>** y **<span style=\"color:green\">RelevantMemories</span>**. Cada una de estas funciones de memoria cumple un propósito distinto en la gestión y recuperación del historial de conversaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Setup inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Instalar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai\n",
    "#! pip install numpy\n",
    "#! pip install tenacity\n",
    "#! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.- Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy\n",
    "from datetime import datetime\n",
    "from llm import generate_text, count_tokens\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.- Variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets and config from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "print(\"OpenAI API key: {}\".format(openai.api_key[:5] + '...' + openai.api_key[-5:]))\n",
    "\n",
    "# Model endpoint names\n",
    "gpt35_model = os.getenv(\"OPENAI_GPT35_MODEL\")\n",
    "gpt35_16k_model = os.getenv(\"OPENAI_GPT35_16K_MODEL\")\n",
    "gpt4_model = os.getenv(\"OPENAI_GPT4_MODEL\")\n",
    "print(\"GPT-3.5-Turbo model: {}\".format(gpt35_model))\n",
    "print(\"GPT-3.5-Turbo-16k model: {}\".format(gpt35_16k_model))\n",
    "print(\"GPT-4 model: {}\".format(gpt4_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.- Importar clases a comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.memory.last_memories import LastMemories\n",
    "from agents.memory.last_tokens import LastTokens\n",
    "from agents.memory.relevant_memories import RelevantMemories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Definición de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.- Función para generar una entrada del usuario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_input(model, conversation):\n",
    "    if len(conversation) == 1:\n",
    "        prompt = \"Inicia la conversación planteando un tema entretenido o haciendo una pregunta tipo {prompt}. Evita temas de IA y tecnología, sé mas creativo. Imagina que estamos teniendo una charla y que compartimos información gradualmente. Evita proporcionar demasiada información en tu primera pregunta y permite que la conversación se desarrolle de manera natural. Evita preguntas sobre gustos personales, juicios, experiencias personales o detalles específicos. Ten en cuenta que el receptor no posee información personal. Responde solo con la prompt.\"\n",
    "        response = generate_text(prompt, model=model)\n",
    "    else:\n",
    "        prompt = \"Genera un prompt para continuar con la conversación, haciendo preguntas adicionales para ahondar en el tema. Y de vez en cuando, cambia a un nuevo tema. La idea es que la conversación sea una especie de entrevista. Evita proporcionar demasiada información en la pregunta y anima a la IA a compartir sus conocimientos sobre algún tema. Evita preguntas sobre gustos personales, juicios, experiencias personales o detalles específicos, el receptor no tiene información personal. Evita temas de IA y tecnología, sé mas creativo y variado al momento de cambiar de tema. Responde solo con la prompt.\"\n",
    "        response = generate_text(prompt, model=model, messages=conversation)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.- Función para simular una conversación y mostrarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_and_display_conversation(model):\n",
    "    conversation = [{\"role\": \"system\", \"content\": \"Entrega siempre respuestas completas, pero no tan extensas\"}]\n",
    "\n",
    "    for i in range(15):  # Simular una conversación de 15 turnos\n",
    "        user_input = generate_user_input(model, conversation)\n",
    "        response = generate_text(user_input, model=model, messages=conversation, max_tokens=2000)\n",
    "        \n",
    "        conversation.append({\"role\": \"user\", \"content\" : user_input})\n",
    "        conversation.append({\"role\": \"assistant\", \"content\" : response})\n",
    "            \n",
    "    # Mostrar la conversación en formato HTML\n",
    "    display(HTML(\"<h3>Conversación generada:</h3>\"))\n",
    "    display(HTML('<table><tr><th>Rol</th><th>Texto</th></tr>' + ''.join([f'<tr><td>{d[\"role\"]}</td><td>{d[\"content\"]}</td></tr>' for d in conversation]) + '</table>'))\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Generar conversación y almacenarla en las respectivas clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_memories = LastMemories()\n",
    "last_tokens = LastTokens()\n",
    "relevant_memories = RelevantMemories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = simulate_and_display_conversation(gpt4_model) #Se pueden utilizar más modelos = gpt35_model, gpt35_16k_model o gpt4_model\n",
    "for message in conversation:\n",
    "    last_memories.add_to_memory(message[\"role\"], message[\"content\"])\n",
    "    last_tokens.add_to_memory(message[\"role\"], message[\"content\"])\n",
    "    relevant_memories.add_to_memory(message[\"role\"], message[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temas tratados en la conversación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Considera la conversación y genera una lista con los temas tratados en ella. Responde solo con la lista y conceptos generales.\"\n",
    "response = generate_text(prompt, model=gpt4_model, messages=conversation, max_tokens = 500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = 0\n",
    "for message in conversation:\n",
    "    tokens += count_tokens(message[\"content\"])\n",
    "display(HTML(f'<h3>Total de tokens de la conversación: {tokens}</h3>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Comparación de las funciones de Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.- Last Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<h3>Resultado de la obtención de Last Memories :</h3>\"))\n",
    "display(HTML('<table><tr><th>Rol</th><th>Texto</th></tr>' + ''.join([f'<tr><td>{d[\"role\"]}</td><td>{d[\"content\"]}</td></tr>' for d in last_memories.get_last_memories()]) + '</table>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.- Last Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<h3>Resultado de la obtención de Last Tokens:</h3>\"))\n",
    "display(HTML('<table><tr><th>Rol</th><th>Texto</th></tr>' + ''.join([f'<tr><td>{b[\"role\"]}</td><td>{b[\"content\"]}</td></tr>' for b in last_tokens.get_last_tokens()]) + '</table>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.- Relevant Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiendo la query\n",
    "prompt = \"Considera la conversación y selecciona un concepto cualquiera abordado en ella. Retorna sólo el concepto.\"\n",
    "query = generate_text(prompt, model=gpt4_model, messages=conversation)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<h3>Resultado de la obtención de Relevant Memories basado en una query:</h3>\"))\n",
    "display(HTML('<table><tr><th>Rol</th><th>Texto</th></tr>' + ''.join([f'<tr><td>{d[\"role\"]}</td><td>{d[\"content\"]}</td></tr>' for d in relevant_memories.get_relevant_memories(query)]) + '</table>'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
